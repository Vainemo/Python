监督学习：
   如果样本带有预先设定的的标签，就说我们正在进行监督学习
   监督学习有两种类型：1.分类 2.回归
   分类：分类是指遍历一个给定的类别集合，之后找到最适合描述特定输入的类别
   回归：回归是指通过一组测量值来预测一些其他的值（通常是下一个值，但也可能是在集合开始之前或中间的某个地方的数值）

无监督学习：
   当输入数据没有标签时，从这些数据中学习的算法均称为无监督学习
   无监督学习通常用于解决我们称之为“聚类”“降噪”和“降维”的问题
   聚类：将一组数据按照相似性分为不同的类
   降噪：去掉样本中的不确定性或其他失真。
   降维：去掉一些不需要关注的特征，减少我们描述数据所需要的值（或维度）

生成器：
    生成器是处于中间地带的，它不需要标签，但是又从我们这里得到了一些反馈，因此我们把这一中间地带称为半监督学习

强化学习：
   强化学习不同于监督学习，因为数据没有标签，虽然从错误中学习这一总体思路是相同的，但是作用的机制不同
   相比之下，在监督学习中，由系统生成一个结果（通常是一个类别或一个预测值），然后我们将其与所提供的正确
   结果进行比较；而在强化学习中，是没有正确结果的，因为数据没有标签，而是通过反馈来告诉我们做得有多好
   特点：环境（道路反馈）会向智能体（自动驾驶的汽车）发送一个奖励信号，这个奖励信号是一个程度值

深度学习：
   基于一系列的离散的层（layer）构建机器学习算法。如果将这些层垂直堆叠，就说这个结果是有深度
  （depth）的，或者说算法是有深度的。
  泛化误差：系统被部署时，其预测新数据标签如果大部分是正确的，那么我们就说它具有很高的准确率，或者说泛化误差（generalization error）很小。
  训练误差：训练样本时网络状态的某种总体平均值。
  测试误差：测试集中的误差。
  验证误差：验证集中的误差

贝叶斯
   似然：P(D∣θ)事件θ发生的情况下D发生的概率
   先验：P(θ) 是先验分布，表示在观测数据之前对参数θ的不确定性的概率分布（在抛硬币之前拿到不公平硬币的概率）。
   后验：P(θ∣D)表示在给定数据 D后参数 θ 的条件概率分布（抛一次硬币之后正面朝上，此时手中时公平或不公平硬币的概率分布）
   证据：P(D) 指事件D以任何方式发生的概率(可能挑选的每个硬币正面朝上的概率的和)

数据验证：
    1.想要知道一个系统在新的、未知的数据上能做得多好，首选的方法就是给它新的、未知的数据，
      我们称这些新的数据点或样本为测试数据（test data）或测试集
    2.训练或测试规则：1.我们决不从测试数据中学习，因为会破坏我们对系统总体能力进行评估的可靠性
    3.系统用测试数据进行学习，被称之为数据泄露（data leakage），也称为数据污染（data contamination）或处理受污染数据
    3.通常将原始输入数据拆分为训练集(用来训练系统，学习特征)，验证集（验证系统性能，选择效果最好的超参数，用作学习过程的一部分），测试集（验证系统性能）

一般流程：
      对于超参数的每一次变化，我们都会训练训练集，而后在验证集上评估系统的性能，之后会选择一组实现效
      果最好的超参数，并通过在从未见过的测试集上运行系统来评估系统的性能。

使用验证数据的结果作为系统的最终评估结果的问题：
      会导致数据泄露，因为尽管分类器没有直接地从验证数据中学习，但是验证数据影响了我们对分类器的选择。我们选择了一个在验证数据
      上表现最好的分类器，在正式应用前，我们对分类器在验证数据上的表现的了解将“泄露”到最终的评估情况中

交叉验证：（当数据量过少时）
      每次训练时将训练数据分割为临时训练集和临时验证集时，这些数据被分割为新的集，这样每次的数据都是全新的，未知的，
      所以用它来评估分类器的性能是公平的。

K折叠交叉验证：
      将训练数据拆分为1-5块，通过索引循环，每次拿1块当验证集，其余4块作为训练集，这样我们就可以将所有数据都拿来训练

过拟合：
      如果系统在训练数据中学习得很多且表现很好，但在面对新数据时表现得很糟糕，我们就可以说系统是过拟合的

过拟合的原因：
       我们“过多地学习了”或者说“过多地适应了”输入的数据。换句话说，我们从它们中学习得太多了，对一些细节权重过大，导致细节代替了普
      遍规则。

过拟合的解决办法：
       1.通过正则化的方法，我们可以鼓励系统尽可能长时间地学习普遍的规则，而不是去拼命地记住细节
       2.我们可以在捕捉到系统开始记忆这些细节的时候，停止它的学习过程。

欠拟合：
       描述了一种规则太过模糊或者普遍的情况，导致泛化误差过高。

正则化方法：
       1.把各个特征的权重压缩至比较小的数值，使其不会出现某些特征权重过大而成为一般规则，λ（lambda）来表示正则
      化量，越大说明正则化程度越高。

偏差：
       1.在一组数据中，各个数据表现简单相似，没有足够的灵活度，这样的一组数据就表现出了高偏差。

方差：
        2.在一组数据中，各个数据表现复杂切不相同，则它们有着高方差。

感知机：
      有若干个输入值，每个输入值对应一个权重和，权重可自行变化，将输入值✖权重之和加上偏置值作为检测值，经过检测之后输出特定的值

激活函数：
      在感知机的输出处，用一个数学函数替代了整个检测再输出的步骤——它将和值（包括偏置）作为输入，然后返回一个新的值作为输出
      这个函数被称之为激活函数.
偏置：
     偏置是加在加权输入之和结果上的单个数字
偏置技巧：
     将偏置重新标记为输入，将它作为一项输入项
     
-----------------------------------scikit-learn--------------------------------------------------------------------
fit():
      每个估算器都会提供的。它需要两个强制参数，包含估算器将从中学习的样本以及与它们相关联的值（或目标）。

predict()：
      评估函数，一旦训练了估算器，我们就可以要求它用predict()来评估新的样本。这至少需要一个参数，那就是新的样本，我们希望估算器为其赋
      值。例程会返回描述新数据的信息，通常这是一个NumPy数组，每个样本对应一个数字或一个类

decision_function()：
      同样是评估函数，与predict不同，decision_function将一组样本作为输入，并为每个类和每个输入返回“confidence”分数，其中较大的值表示更高的可信度

predict_proba()：
      输出概率，所有分类结果概率和为1，比decision_function()更加直观。

RidgeClassifier：
      分类器，使用Ridge回归算法的分类器版本

RidgeClassifierCV：
      内置交叉方差的RidgeClassifier

集成器：
     估算器的集合称为集成器，使用AdaBoostClassifier()创建集成器，AdaBoostClassifier(RidgeClassifier(), \algorithm=′SAMME′)包含一个分类器和算法参数

变换器（transform）：
      scikit-learn提供了各种各样的对象，称为变换器，它可以执行许多不同类型的数据变换。每个变换器接收一个包含样本数据的NumPy数组，并返回一个变换数组。

pipeline：
      它允许将多个数据处理步骤组合成一个整体，使得数据从原始状态经过一系列的转换和处理后，最终输入到模型中进行训练或预测(预处理对象和分类对象)。

PolynomialFeatures：
      用于进行多项式特征生成的工具。它可以将原始特征集扩展为包含原始特征的各种幂和交互项的集合，从而使模型能够更好地拟合非线性关系，
      参数degree表示多项式的最高阶数，可包含<=degree的所有次幂的特征
---------------------------------------------------------------------------------------------------------
      
-------------------------------------------前馈网络------------------------------------------------------
前馈网络特征：
       使信息仅朝一个方向流动，数据一旦离开节点，就不会返回，最常见的网络结构是排列神经元

DAG(有向无环图):
      神经网络中的一个普遍规则是没有循环（loop）。这意味着来自节点的数据永远不会回到同一个节点，无论它遵循的路径多么迂回

全连接网络:
      全连接网络，其中每个神经元输入来自上一层的所有神经元。

网络崩溃：
       单个神经元所产生的值与复杂网络中的输出值相同。而且结果出现得会更快，占用的计算机内存也更少。那么神经网络永远不会比单个神经元所产生的结果好
       出现这种现象就被称之为网络崩溃。可以使用激活函数(非线性函数：除了加法和乘法以外的函数)来阻止网络崩溃。

非线性环节：
       因为激活函数通常是人工神经元处理过程中唯一的非线性部分，所以激活函数通常被称为非线性环节。

阶跃函数：
       它保持为一个值，直到某个阈值后，会跳转为另一个值；

Heaviside阶跃（Heaviside step）函数：
       阈值是0的阶跃函数

饱和：
       当输入值大于某一个值时，函数将返回相同的值，这种现象被称为饱和;

ReLU(分段线性函数)：
       如果一个函数是由几个部分组成的，而每个部分都是一条直线，那么我们称它是分段线性（piecewise linear）的，它称为修正线性单元
       （rectifiedlinear unit），或者说线性整流函数，使用参数ReLU时，最重要的是永远不要选择1.0的比例因子，因为这样我们就失去了弯折（kink），
       整个函数就是一条直线。

参数ReLU:
       可用于选择缩放的比例.

移位ReLU:
       将函数弯折处向某个方向移动

maxout激活函数：
      结合基本的ReLU和泄漏型ReLU（或参数ReLU），maxout允许我们定义一组直线，每个点的函数的输出是所有直线在那一点求值后最大的  

----------光滑曲线------------
softplus函数：
      将ReLU做了简单的平滑处理

指数式ReLU：
      也被称为ELU，将移位ReLU进行平滑处理。

sigmoid函数：
       1.也称为logistic函数（logistic function）或logistic曲线（logistic curve），“sigmoid函数”这个名字来源于函数曲线与S形的相似   
       2.S形的sigmoid函数也称为逻辑函数或逻辑曲线。对于极负的输入，它的输出值为0；而对于极正的输入，它的输出值为1。
双曲正切（hyperbolic tangent）函数(tanh函数)
       也是S型的，与的sigmoid的区别在于双曲正切函数为非常负的输入值返回−1的输出，而它的过渡区域也会稍微狭窄一点
swish函数：
       基本的ReLU与sigmoid函数的组合，从本质上说，它就是一个ReLU，但是在0的左边出现了一个小而平滑的凹陷，之后函数变平了
光滑曲线缺点：
       它们只能在有限的输入范围内产生有用的结果，在训练神经网络时，导数的值是至关重要的信息，如果导数趋于0，训练就趋于停止
--------------------------------------
softmax函数(归一化指数函数):
       可以把输出项的得分变成概率，所以它被广泛应用于分类器神经网络的末端
------------------------------------------------------------------------------------------------------------------



--------------------------------------------------反向传播---------------------------------------------------------
          神经网络从误差中学习。每次系统进行不正确的预测时，我们都会使用一种称为反向传播的算法来改善其权重



